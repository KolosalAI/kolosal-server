server:
  port: "8084"
  host: "0.0.0.0"
  max_connections: 100
  worker_threads: 0
  request_timeout: 30
  max_request_size: 16777216
  idle_timeout: 300
  allow_public_access: false  # Set to true to allow access from other devices on your network
  allow_internet_access: false  # Set to true to enable internet access (requires port forwarding)

logging:
  level: "INFO"
  file: ""
  access_log: false

database:
  # Qdrant vector database configuration
  qdrant:
    enabled: true
    host: "localhost"
    port: 6333
    collection_name: "documents"
    default_embedding_model: "text-embedding-3-small"  # Model to use for embedding documents
    timeout: 30
    # Optional authentication
    api_key: ""
    # Connection pooling
    max_connections: 10
    connection_timeout: 5

auth:
  enabled: true
  require_api_key: false
  api_key_header: "X-API-Key"
  api_keys: 
    - "your_api_key_here"
    - "sk-1234567890abcdef"
  rate_limit:
    enabled: true
    max_requests: 100
    window_size: 60
  cors:
    enabled: true
    allow_credentials: false
    max_age: 86400
    allowed_origins:
      - "*"
    allowed_methods:
      - "GET"
      - "POST"
      - "PUT"
      - "DELETE"
      - "OPTIONS"
      - "HEAD"
      - "PATCH"
    allowed_headers:
      - "Content-Type"
      - "Authorization"
      - "X-Requested-With"
      - "Accept"
      - "Origin"

models:
  - id: "qwen3-0.6b"
    type: "llm"  # llm or embedding
    path: "https://huggingface.co/kolosal/qwen3-0.6b/resolve/main/Qwen3-0.6B-UD-Q4_K_XL.gguf"
    load_at_startup: true
    main_gpu_id: 0
    preload_context: false
    load_params:
      n_ctx: 2048
      n_keep: 1024
      use_mmap: true
      use_mlock: false
      n_parallel: 1
      cont_batching: true
  - id: "gpt-3.5-turbo"
    type: "llm"
    path: "./models/gpt-3.5-turbo.gguf"
    load_at_startup: false
    main_gpu_id: 0
    preload_context: false
    load_params:
      n_ctx: 4096
      n_keep: 2048
      use_mmap: true
      use_mlock: false
      n_parallel: 1
      cont_batching: true
  - id: "text-embedding-3-small"
    type: "embedding"
    path: "./models/all-MiniLM-L6-v2-Q4_K_M.gguf"
    load_at_startup: true
    main_gpu_id: 0
    preload_context: false
    embedding_config:
      dimensions: 384
      normalize: true
      pooling_type: "mean"  # mean, cls, or none
      max_input_length: 8192
    load_params:
      n_ctx: 512
      n_keep: 0
      use_mmap: true
      use_mlock: false
      n_parallel: 4  # Higher parallelism for embedding models
      cont_batching: true
      n_batch: 512
      n_ubatch: 128
  - id: "mini-lm-l6-v2"
    type: "embedding"
    path: "https://huggingface.co/Mungert/all-MiniLM-L6-v2-GGUF/blob/main/all-MiniLM-L6-v2-bf16-q4_k.gguf"
    load_at_startup: false
    main_gpu_id: 0
    preload_context: false
    embedding_config:
      dimensions: 768
      normalize: true
      pooling_type: "mean"
      max_input_length: 8192
    load_params:
      n_ctx: 512
      n_keep: 0
      use_mmap: true
      use_mlock: false
      n_parallel: 2
      cont_batching: true
      n_batch: 256
      n_ubatch: 64

# Embedding-specific autoscaling configuration
embedding_autoscaling:
  enabled: true
  min_instances: 1
  max_instances: 4
  scale_up_threshold: 10  # requests per second
  scale_down_threshold: 2  # requests per second
  scale_up_delay: 30      # seconds
  scale_down_delay: 300   # seconds (5 minutes)
  check_interval: 15      # seconds

features:
  health_check: true
  metrics: true
